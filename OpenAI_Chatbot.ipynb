{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai chromadb gradio pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîê Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"  # Replace with your actual key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdce6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path, persist_directory=\"chroma_db\"):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(chunks, embedding=embedding, persist_directory=persist_directory)\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b63a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dac722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_pdf(file):\n",
    "    global qa_chain\n",
    "    vectordb = process_pdf(file.name)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb.as_retriever())\n",
    "    return \"‚úÖ PDF processed. You can now ask questions!\"\n",
    "\n",
    "def ask_question(question):\n",
    "    if qa_chain is None:\n",
    "        return \"‚ùå Please upload and process a PDF first.\"\n",
    "    return qa_chain.run(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# üìÑ Chat with your PDF (OpenAI + LangChain)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload PDF\")\n",
    "        upload_btn = gr.Button(\"Process PDF\")\n",
    "        status = gr.Textbox(label=\"Status\")\n",
    "\n",
    "    upload_btn.click(fn=upload_pdf, inputs=[file_input], outputs=[status])\n",
    "\n",
    "    with gr.Row():\n",
    "        query = gr.Textbox(label=\"Your Question\")\n",
    "        ask_btn = gr.Button(\"Ask\")\n",
    "        answer = gr.Textbox(label=\"Answer\")\n",
    "\n",
    "    ask_btn.click(fn=ask_question, inputs=[query], outputs=[answer])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
